---
description: 2025.05.05.
---

# LLM은 더우면 헛소리를 해?

<figure><img src="../.gitbook/assets/그림1.png" alt="" width="375"><figcaption><p>에휴..</p></figcaption></figure>



흔히 이런말을 들어본적이 있을 것이다.&#x20;

> **" LLM이 응답을 할 때 temperature가 높으면, 할루시네이션이 높은 답변이 나와!! "**

이 말은 과연  올바른 말인지 생각해본적이 있나? 필자의 경우 무심코 **"그치, 그말이 맞지"**&#xB77C;고 오랫동안  생각했었던 것 같다. 오늘은 방금말한 내용이 정말 올바른 말인지 알아보고, 상황에 따라 Temperature값, 그리고 그에 준하는 다른 파라미터들을 어떻게 적절하게 조절하면 되는지 한번 생각해보자.

## Ⅰ. LLM이 답변을 생성하는 원리

일단 가장 먼저 LLM이 답변을 생성하는 원리에 대해 짚고 넘어가자. 아주 단순하게 비유하면 <mark style="color:green;">**LLM은 주어진 말에이어서 나오기 가장 적당한 단어를 LLM이 알고있는 단어사전에서 선택하는 행동을 반복하는 프로그램**</mark>이다.\
그리고 단어를 선택하는 근본적인 원리가 확률분포이고, 이 확률분포에 영향을 미치는 대표적인 파라미터 값이 **Temperature, Top-K, Top-P**이다.&#x20;

실제로 LLM의 원리에 입각하여 Next Token(=다음단어)을 예측하는데 실질적인 과정은 다음 단계를 따른다.

### 1. 입력 & Logit 값 생성

LLM의 입력 값으로 들어온 컨텍스트에 기반해 사전상의 모든 토큰에 점수를 매깁니다. 이 값은 로짓(Logit)값 이라고 합니다. 만약 LLM에 "한국의 수도는"이라는 컨텍스트가 입력이 되었다고 가정하고 생성된 토큰별 로짓값이 아래와 같다고 상정해보겠습니다.

* 서울 : 12.4
* 부산 : 8.2
* 인천 : 5.1
* 기타 : ...

### 2. Temperature 연산 & 확률분포 생성

토큰별 logit값을 temperature 값으로 나누어 정규화해줍니다. 만약 temperature가 1이면 아무 영향이 없고, 1보다 크면 각 토큰간 점수 차이가 작아지며, 1보다 작으면 차이가 커질 것 입니다. 이를 풀어서 얘기하면 temperature가 작을수록&#x20;

1. Logit 값 생성 (e.g. Logit Value = 서울 : 12.4, 부산 : 8.2, 인천 : 5.1, ... )
2. Temperature 연산 (e.g. Scaled Logit = 서울 : 17.7, 부산 : 11.7, 인천 : 7.3, ... )
3. 확률분포 생성 (e.g. Softmax 생성 = 서울 70%, 부산 20%, 인천 5%, ... )
4. Top-K 필터링 (e.g. top-k=3, 최대 3개 토큰만 남기고 정규화 = 서울 74%, 부산 21%, 인청 5% )
5. Top-P 필터링 (e.g. top-p=0.95, 누적확률이 0.95가 될 때까지 토큰 선택 후 다시 정규화 = 서울 78%, 부산 22% )
6. 샘플링 후 출력 (e.g. 한국의 수도는 서울)

## Ⅱ. 상황에 따라 조절하는 방법
