---
description: GloVe(2014)ì™€ FastText(2016)ì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤.
---

# ğŸ•·ï¸ GloVe

## â… . GloVe

ê¸€ë¡œë¸Œ(Global Vectors for Word Representation, GloVe)ëŠ” ì¹´ìš´íŠ¸ ê¸°ë°˜ê³¼ ì˜ˆì¸¡ ê¸°ë°˜ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ 2014ë…„ì— ë¯¸êµ­ ìŠ¤íƒ í¬ë“œëŒ€í•™ì—ì„œ ê°œë°œí•œ ë‹¨ì–´ ì„ë² ë”© ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ì•ì„œ í•™ìŠµí•˜ì˜€ë˜ ê¸°ì¡´ì˜ ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ LSA(Latent Semantic Analysis)ì™€ ì˜ˆì¸¡ ê¸°ë°˜ì˜ Word2Vecì˜ ë‹¨ì ì„ ì§€ì í•˜ë©° ì´ë¥¼ ë³´ì™„í•œë‹¤ëŠ” ëª©ì ìœ¼ë¡œ ë‚˜ì™”ê³ , ì‹¤ì œë¡œë„ Word2Vecë§Œí¼ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ì˜ ì—°êµ¬ì— ë”°ë¥´ë©´ ë‹¨ì •ì ìœ¼ë¡œ Word2Vecì™€ GloVe ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì´ ë” ë›°ì–´ë‚˜ë‹¤ê³  ë§í•  ìˆ˜ëŠ” ì—†ê³ , ì´ ë‘ ê°€ì§€ ì „ë¶€ë¥¼ ì‚¬ìš©í•´ë³´ê³  ì„±ëŠ¥ì´ ë” ì¢‹ì€ ê²ƒì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•©ë‹ˆë‹¤.

{% code lineNumbers="true" %}
```bash
!pip install glove-python3
```
{% endcode %}

{% code lineNumbers="true" %}
```python
from glove import Corpus, Glove
import re
import urllib.request
import zipfile
from lxml import etree
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize, sent_tokenize

# ë°ì´í„° ë‹¤ìš´ë¡œë“œ
urllib.request.urlretrieve("https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml", filename="ted_en-20160408.xml")

targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')
target_text = etree.parse(targetXML)

# xml íŒŒì¼ë¡œë¶€í„° <content>ì™€ </content> ì‚¬ì´ì˜ ë‚´ìš©ë§Œ ê°€ì ¸ì˜¨ë‹¤.
parse_text = '\n'.join(target_text.xpath('//content/text()'))

# ì •ê·œ í‘œí˜„ì‹ì˜ sub ëª¨ë“ˆì„ í†µí•´ content ì¤‘ê°„ì— ë“±ì¥í•˜ëŠ” (Audio), (Laughter) ë“±ì˜ ë°°ê²½ìŒ ë¶€ë¶„ì„ ì œê±°.
# í•´ë‹¹ ì½”ë“œëŠ” ê´„í˜¸ë¡œ êµ¬ì„±ëœ ë‚´ìš©ì„ ì œê±°.
content_text = re.sub(r'\([^)]*\)', '', parse_text)

# ì…ë ¥ ì½”í¼ìŠ¤ì— ëŒ€í•´ì„œ NLTKë¥¼ ì´ìš©í•˜ì—¬ ë¬¸ì¥ í† í°í™”ë¥¼ ìˆ˜í–‰.
sent_text = sent_tokenize(content_text)

# ê° ë¬¸ì¥ì— ëŒ€í•´ì„œ êµ¬ë‘ì ì„ ì œê±°í•˜ê³ , ëŒ€ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜.
normalized_text = []
for string in sent_text:
     tokens = re.sub(r"[^a-z0-9]+", " ", string.lower())
     normalized_text.append(tokens)

# ê° ë¬¸ì¥ì— ëŒ€í•´ì„œ NLTKë¥¼ ì´ìš©í•˜ì—¬ ë‹¨ì–´ í† í°í™”ë¥¼ ìˆ˜í–‰.
result = [word_tokenize(sentence) for sentence in normalized_text]
print('ì´ ìƒ˜í”Œì˜ ê°œìˆ˜ : {}'.format(len(result)))
```
{% endcode %}

```
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
ì´ ìƒ˜í”Œì˜ ê°œìˆ˜ : 273424
```

{% code lineNumbers="true" %}
```python
corpus = Corpus() 
corpus.fit(result, window=5)
# í›ˆë ¨ ë°ì´í„°ë¡œë¶€í„° GloVeì—ì„œ ì‚¬ìš©í•  ë™ì‹œ ë“±ì¥ í–‰ë ¬ ìƒì„±

glove = Glove(no_components=100, learning_rate=0.05)
glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)
glove.add_dictionary(corpus.dictionary)
# í•™ìŠµì— ì´ìš©í•  ì“°ë ˆë“œì˜ ê°œìˆ˜ëŠ” 4ë¡œ ì„¤ì •, ì—í¬í¬ëŠ” 20.
```
{% endcode %}

```
Performing 20 training epochs with 4 threads
Epoch 0
Epoch 1
Epoch 2
Epoch 3
Epoch 4
Epoch 5
Epoch 6
Epoch 7
Epoch 8
Epoch 9
Epoch 10
Epoch 11
Epoch 12
Epoch 13
Epoch 14
Epoch 15
Epoch 16
Epoch 17
Epoch 18
Epoch 19
```

{% code lineNumbers="true" %}
```python
model_result1=glove.most_similar("man")
print(model_result1)
```
{% endcode %}

```
[('woman', 0.9604732446102091), ('guy', 0.8963648521480801), ('girl', 0.8755874049593755), ('boy', 0.8466197641507356)]
```

## â…¡. FastText

ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë§Œë“œëŠ” ë˜ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” í˜ì´ìŠ¤ë¶ì—ì„œ ê°œë°œí•œ FastTextê°€ ìˆìŠµë‹ˆë‹¤. Word2Vec ì´í›„ì— ë‚˜ì˜¨ ê²ƒì´ê¸° ë•Œë¬¸ì—, ë©”ì»¤ë‹ˆì¦˜ ìì²´ëŠ” Word2Vecì˜ í™•ì¥ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Word2Vecì™€ FastTextì™€ì˜ ê°€ì¥ í° ì°¨ì´ì ì´ë¼ë©´ Word2VecëŠ” ë‹¨ì–´ë¥¼ ìª¼ê°œì§ˆ ìˆ˜ ì—†ëŠ” ë‹¨ìœ„ë¡œ ìƒê°í•œë‹¤ë©´, FastTextëŠ” í•˜ë‚˜ì˜ ë‹¨ì–´ ì•ˆì—ë„ ì—¬ëŸ¬ ë‹¨ì–´ë“¤ì´ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤. ë‚´ë¶€ ë‹¨ì–´. ì¦‰, ì„œë¸Œì›Œë“œ(subword)ë¥¼ ê³ ë ¤í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.

FastTextì—ì„œëŠ” ê° ë‹¨ì–´ëŠ” ê¸€ì ë‹¨ìœ„ n-gramì˜ êµ¬ì„±ìœ¼ë¡œ ì·¨ê¸‰í•©ë‹ˆë‹¤. nì„ ëª‡ìœ¼ë¡œ ê²°ì •í•˜ëŠ”ì§€ì— ë”°ë¼ì„œ ë‹¨ì–´ë“¤ì´ ì–¼ë§ˆë‚˜ ë¶„ë¦¬ë˜ëŠ”ì§€ ê²°ì •ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ì„œ nì„ 3ìœ¼ë¡œ ì¡ì€ íŠ¸ë¼ì´ê·¸ë¨(tri-gram)ì˜ ê²½ìš°, appleì€ app, ppl, pleë¡œ ë¶„ë¦¬í•˜ê³  ì´ë“¤ì„ ë²¡í„°ë¡œ ë§Œë“­ë‹ˆë‹¤. ë” ì •í™•íˆëŠ” ì‹œì‘ê³¼ ëì„ ì˜ë¯¸í•˜ëŠ” <, >ë¥¼ ë„ì…í•˜ì—¬ ì•„ë˜ì˜ 5ê°œ ë‚´ë¶€ ë‹¨ì–´(subword) í† í°ì„ ë²¡í„°ë¡œ ë§Œë“­ë‹ˆë‹¤.&#x20;

ë§Œì•½n = 3ì¸ ê²½ìš° \<ap, app, ppl, ple, le> ê³¼ ê°™ì´ ìª¼ê°œì§‘ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì—¬ê¸°ì— ì¶”ê°€ì ìœ¼ë¡œ í•˜ë‚˜ë¥¼ ë” ë²¡í„°í™”í•˜ëŠ”ë°, ê¸°ì¡´ ë‹¨ì–´ì— <, ì™€ >ë¥¼ ë¶™ì¸ \<apple> í† í°ì…ë‹ˆë‹¤.

FastTextì˜ ì¸ê³µ ì‹ ê²½ë§ì„ í•™ìŠµí•œ í›„ì—ëŠ” ë°ì´í„° ì…‹ì˜ ëª¨ë“  ë‹¨ì–´ì˜ ê° n-gramì— ëŒ€í•´ì„œ ì›Œë“œ ì„ë² ë”©ì´ ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë˜ë©´ ì¥ì ì€ ë°ì´í„° ì…‹ë§Œ ì¶©ë¶„í•œë‹¤ë©´ ìœ„ì™€ ê°™ì€ ë‚´ë¶€ ë‹¨ì–´(Subword)ë¥¼ í†µí•´ ëª¨ë¥´ëŠ” ë‹¨ì–´(Out Of Vocabulary, OOV)ì— ëŒ€í•´ì„œë„ ë‹¤ë¥¸ ë‹¨ì–´ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ê°€ë ¹, FastTextì—ì„œ birthplace(ì¶œìƒì§€)ë€ ë‹¨ì–´ë¥¼ í•™ìŠµí•˜ì§€ ì•Šì€ ìƒíƒœë¼ê³  í•´ë´…ì‹œë‹¤. í•˜ì§€ë§Œ ë‹¤ë¥¸ ë‹¨ì–´ì—ì„œ birthì™€ placeë¼ëŠ” ë‚´ë¶€ ë‹¨ì–´ê°€ ìˆì—ˆë‹¤ë©´, FastTextëŠ” birthplaceì˜ ë²¡í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¥´ëŠ” ë‹¨ì–´ì— ì œëŒ€ë¡œ ëŒ€ì²˜í•  ìˆ˜ ì—†ëŠ” Word2Vec, GloVeì™€ëŠ” ë‹¤ë¥¸ ì ì…ë‹ˆë‹¤.

ì½”ë“œ êµ¬í˜„ì„ í†µí•´ FastTextì™€ Word2Vecì„ ë¹„êµí•´ë´…ì‹œë‹¤.

{% code lineNumbers="true" %}
```python
import re
import urllib.request
import zipfile
from lxml import etree
from nltk.tokenize import word_tokenize, sent_tokenize
from gensim.models import FastText, Word2Vec, KeyedVectors

# ë°ì´í„° ë‹¤ìš´ë¡œë“œ
urllib.request.urlretrieve("https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml", filename="ted_en-20160408.xml")

targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')
target_text = etree.parse(targetXML)

# xml íŒŒì¼ë¡œë¶€í„° <content>ì™€ </content> ì‚¬ì´ì˜ ë‚´ìš©ë§Œ ê°€ì ¸ì˜¨ë‹¤.
parse_text = '\n'.join(target_text.xpath('//content/text()'))

# ì •ê·œ í‘œí˜„ì‹ì˜ sub ëª¨ë“ˆì„ í†µí•´ content ì¤‘ê°„ì— ë“±ì¥í•˜ëŠ” (Audio), (Laughter) ë“±ì˜ ë°°ê²½ìŒ ë¶€ë¶„ì„ ì œê±°.
# í•´ë‹¹ ì½”ë“œëŠ” ê´„í˜¸ë¡œ êµ¬ì„±ëœ ë‚´ìš©ì„ ì œê±°.
content_text = re.sub(r'\([^)]*\)', '', parse_text)

# ì…ë ¥ ì½”í¼ìŠ¤ì— ëŒ€í•´ì„œ NLTKë¥¼ ì´ìš©í•˜ì—¬ ë¬¸ì¥ í† í°í™”ë¥¼ ìˆ˜í–‰.
sent_text = sent_tokenize(content_text)

# ê° ë¬¸ì¥ì— ëŒ€í•´ì„œ êµ¬ë‘ì ì„ ì œê±°í•˜ê³ , ëŒ€ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜.
normalized_text = []
for string in sent_text:
     tokens = re.sub(r"[^a-z0-9]+", " ", string.lower())
     normalized_text.append(tokens)

# ê° ë¬¸ì¥ì— ëŒ€í•´ì„œ NLTKë¥¼ ì´ìš©í•˜ì—¬ ë‹¨ì–´ í† í°í™”ë¥¼ ìˆ˜í–‰.
result = [word_tokenize(sentence) for sentence in normalized_text]

# Word2Vecê³¼ FastTextì˜ ë¹„êµ
model_wv = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)
model_ft = FastText(result, vector_size=100, window=5, min_count=5, workers=4, sg=1)
```
{% endcode %}

{% code lineNumbers="true" %}
```python
model_wv.wv.most_similar("electrofishing")
```
{% endcode %}

```
KeyError: "Key 'electrofishing' not present in vocabulary"
```

{% code lineNumbers="true" %}
```python
model_ft.wv.most_similar("electrofishing")
```
{% endcode %}

```
[('electrolyte', 0.8595626950263977),
 ('electrolux', 0.8584996461868286),
 ('electroshock', 0.8453683853149414),
 ('electro', 0.8443098068237305),
 ('electrochemical', 0.8417826294898987),
 ('electroencephalogram', 0.8407151699066162),
 ('electrogram', 0.8311992883682251),
 ('airbus', 0.82285076379776),
 ('airbag', 0.8175927996635437),
 ('electrons', 0.8162853717803955)]
```
