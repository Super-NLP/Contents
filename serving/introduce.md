---
description: ì–¸ì–´ëª¨ë¸ê³¼ LLMì„ í¬í•¨í•˜ì—¬ ì–´ë–»ê²Œ ì„œë¹„ìŠ¤ë¡œ ì—°ê²°ì‹œì¼œ ì„œë¹™í•˜ëŠ”ì§€ì— ëŒ€í•´ ì•Œì•„ë³´ì.
coverY: 0
---

# Introduce

> **Keyword**\
> \-> Docker, Kubernates(k8s), FastAPI, MLflow, vLLM, Lakehouse, Blue/Green, LoadBalancing

### **âœ… How to serve LLM in real service**

* **Prototype** or **PoC Level**
  * ì„œë¹™ë„ FastAPI + Uvicorn í™œìš©í•´ì„œ ê°„ë‹¨í•˜ê²Œ ìˆ˜í–‰ ê°€ëŠ¥
* **Product Level**
  * ê³µí†µì ìœ¼ë¡œ Service (Server) â†” Model (Proxy Server + Docker + Kubernatesê°€ ì •ì„ìœ¼ë¡œ í™œìš©ë˜ëŠ”ë“¯ í•¨. PyTorch, Tensorflow, Langchain, Ollama, Azure Databricks, MLFlow, Lakehouse, etcâ€¦ ì—¬ëŸ¬ê°€ì§€ ë°°í¬í™˜ê²½ì— ë”°ë¼ ì‚¬ìš©í•˜ëŠ” í”„ë ˆì„ì›Œí¬ëŠ” ë‹¤ì–‘í•˜ê²Œ ë‚˜ë‰  ê²ƒìœ¼ë¡œ ë³´ì„.
  * **On-premise í™˜ê²½**
    * ì˜¨í”„ë ˆë¯¸ìŠ¤ í™˜ê²½ì€ ì›ê²©í™˜ê²½ì—ì„œ ì„œë²„ë¥¼ ìš´ì˜í•˜ëŠ” í´ë¼ìš°ë“œì™€ëŠ” ëŒ€ë¹„ë˜ëŠ” ê°œë…ìœ¼ë¡œ **ê¸°ì—…ì´ ì„œë²„ë¥¼ ìì²´ì ìœ¼ë¡œ ë³´ìœ í•˜ê³  ì§ì ‘ ì„¤ì¹˜ ë° ìš´ì˜í•˜ëŠ” ë°©ì‹**ì„ ì˜ë¯¸í•¨.
    * ë°ì´í„° ë³´ì•ˆì´ë‚˜ ì™¸ë¶€ë§ í™œìš©ì´ ì œí•œë˜ëŠ” ê²½ìš°, í•„ìˆ˜ì ìœ¼ë¡œ LangChain í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•´ì•¼í•  ê²ƒìœ¼ë¡œ íŒë‹¨ë¨
  * **Cloud í™˜ê²½**
    * Azure í´ë¼ìš°ë“œ ê¸°ë°˜ìœ¼ë¡œ **Azure Databricks** ë“±ì˜ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ LangChain ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŒ.



### ğŸ“œ Reference

[LangChain: LLM ì„œë¹„ìŠ¤ë¥¼ ì–´ë–»ê²Œ ê°œë°œí•  ìˆ˜ ìˆì„ê¹Œ?](https://bigwaveai.tistory.com/76)

[ìƒˆë¡œìš´ ë£¨ë‹¤ë¥¼ ì§€íƒ±í•˜ëŠ” ëª¨ë¸ ì„œë¹™ ì•„í‚¤í…ì²˜ â€” 1í¸: A/B í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ êµ¬ì¡° ì„¤ê³„](https://tech.scatterlab.co.kr/serving-architecture-1/)

[ìƒˆë¡œìš´ ë£¨ë‹¤ë¥¼ ì§€íƒ±í•˜ëŠ” ëª¨ë¸ ì„œë¹™ ì•„í‚¤í…ì²˜ â€” 2í¸: ArgoCDì™€ ëª¨ë¸ ì„œë¹™](https://tech.scatterlab.co.kr/serving-architecture-2/)

[HyperCLOVA ì„œë¹™ í”„ë ˆì„ì›Œí¬ ì„ ì • | CLOVA Engineering Blog](https://engineering.clova.ai/posts/2022/01/hyperclova-part-1)

[NSML - ë¶„ì‚° í•™ìŠµ í”Œë«í¼ì˜ ìŠ¤ì¼€ì¤„ë§ ìš”êµ¬ ì‚¬í•­ê³¼ í•´ê²° ë°©ì•ˆ | CLOVA Engineering Blog](https://engineering.clova.ai/posts/2022/08/nsml-scheduler-part-1)

[vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html)
