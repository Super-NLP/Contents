---
description: NLP Engineering, LLM Engineering 직군의 취업을 위해 준비했던 혹은 실제 마주했던 기술인터뷰 목록을 정리해봅시다.
cover: >-
  https://images.unsplash.com/photo-1579548122080-c35fd6820ecb?crop=entropy&cs=srgb&fm=jpg&ixid=M3wxOTcwMjR8MHwxfHNlYXJjaHw0fHxncmFkYXRpb258ZW58MHx8fHwxNzExMTgwMTI2fDA&ixlib=rb-4.0.3&q=85
coverY: 0
---

# Interview List

* MSE는 왜 회귀모델에 유리하고 CrossEntropy는 왜 분류모델에 유리한가요?
* RAG의 장점이 뭐에요?
* Recall, Precision, F1-Score는 각각 무엇을 의미하죠?
* BERT에서 WordPiece Tokenizer를 썼는데, 왜 이것을 사용했고 사용했을 때 장점이 무엇일까요?
* RAG를 구축할 때 ChunkSize를 크게 하면 성능이 좋을까요? 안좋을까요?
* LoRA가 무엇인가요?
* LoRA및 QLoRA를 이용하여 모델을 어떻게 학습시키나요?
* 효율적인 딥러닝 학습/추론 엔진의 개발/분석/개선방법에 대해 아는대로 말해보세요.
* 정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?
* Data Normalization은 무엇이고 왜 필요한가요?
* Vector DB의 장점이 뭐에요?
* Prompt Engineering에 대해 아는대로 설명해주실래요?
* RNN, LSTM과 Transformer의 차이점을 설명해주세요.
* BERT의 \[CLS] 토큰에 대해 설명해주세요.
* Fine Tuning과 SFT에 대해 비교하여 설명해주세요.
* Word Embedding과 Sentence Embedding에 대해 비교하여 설명해주세요.
* Knowledge Distillation에 대해 설명해주세요.
