---
description: >-
  데이터 클리닝에 대하여 알아봅시다. 데이터 정제(cleaning), 정규화(normalization), 전처리(preprocessing)
  모두 의미상 같은 맥락의 작업을 가리킵니다.
---

# 🐌 Data Cleaning

데이터 정제작업은 학습에 필요한 데이터를 정리하는 작업과 같다. 수집한 데이터 그대로 사용하기에는 불필요한 단어, 기호, 공백과 같은 요소가 많기 때문에 필수적으로 진행해야 한다. 원하는 업무에 따라, 또는 응용분야에 따라 필요한 정제의 수준이나 깊이가 다를 수 있다. 한가지 예를 들자면 음성인식을 위한 언어모델의 경우 사람의 음성을 그대로 받아적어야 하므로 괄호 또는 별표와 같은 기호나 특수문자를 엄격하게 체크해야 한다. 그밖에 민감한 정보를 담은 데이터의 경우 변조를 동반하기도 한다. 이렇듯 코퍼스 데이터를 정제하는 방법을 알아보자.

## Ⅰ. Cleaning & Normalization

### ⅰ. 전각문자 제거

전각문자는 하나의 글자가 정사각형의 형상을 띈 문자를 말한다. 이에 반해 반각문자는 전각문자의 가로폭을 절반으로 줄인 문자를 말하는데, 초창기 NLP의 경우 연구 대부분이 영어권에서 이루어졌기에 한글, 한자, 히라가나 등에 해당하는 전각문자를 영어, 숫자가 포함되는 반각문자로 바꿔줄 필요가 있었다.&#x20;

### ⅱ. 대소문자 통일

일부 영어 코퍼스의 경우 약어를 사용할 경우 대소문자에 대한 통일이 이루어지지 않는 경우가 존재했다.

### ⅲ. 정규표현식을 이용한 전처리

일반적으로 정규표현식(Regex)라는 방법을 활용하여 전처리를 수행하는데 통상적으로 얻어낸 코퍼스는 잡다한 노이즈도 많고 내부적으로 일정한 패턴을 지니는 경우도 존재한다. 이때 정규표현식을 사용하면 효과적으로 정제할 수 있다. 많은 내용을 다루기보단 기초적인 문법과 예시를 소개하고 넘어가도록 하겠다.

👉 **정규표현식 시각화 사이트 :** [**https://regexper.com/**](https://regexper.com/\*\*)

* <mark style="color:orange;">**\[ ]**</mark>** : 대괄호 안에 들어가 있는 각 요소를 or로 표기한다.**
  * \[23456cde] → 2 or 3 or 4 or 5 or 6 or c or d or e
* <mark style="color:orange;">**-**</mark>** : 연속된 숫자 또는 알파벳을 표현할 수 있다.**
  * \[2-5c-e] → 2\~5 and c\~e
* <mark style="color:orange;">**\[^]**</mark>** : Not을 ^ 기호로 표현할 수 있다.**
  * \[^2-5c-e] → Not (2\~5 and c\~e)
* <mark style="color:orange;">**( )**</mark>** : 괄호를 이용해 그룹을 만들 수 있다.**
  * (x)(yz) → x와 yz를 각각의 그룹으로 묶는다.
* <mark style="color:orange;">**|**</mark>** : | 기호를 이용하여 or을 표현할 수 있다.**
  * (x|y) → x or y
* \*_?, , + : 각 기호에 따라 의미하는 바가 다르다._
  * <mark style="color:orange;">**?**</mark>** : 앞의 수식하는 부분이 나타나지 않거나 한 번만 나타날 때는 “?”를 사용한다.**
    * x? → x가 나타나지 않거나 한 번만 나타남.
  * <mark style="color:orange;">**+**</mark>** : 앞에 수식하는 부분이 한 번 이상 나타날 때 ‘+’를 사용한다.**
    * x+ → x가 한 번 이상 나타난다.
  * <mark style="color:orange;">\*</mark> : 앞의 수식하는 부분이 나타나지 않거나 여러 번 나타날 때 ‘\*’를 사용한다.
    * x\* → x가 나타나지 않거나 여러번 나타난다.
* **{n}, {n,}, {n,m} : 각 기호에 따라 의미하는 바가 다르다.**
  * <mark style="color:orange;">{n}</mark> : 정확한 반복횟수를 알고있을 때 표현한다.
    * x{n} → x가 n번 나타난다.
  * <mark style="color:orange;">{n,}</mark> : n번 이상 반복할 때 표현한다.
    * x{n,} → x가 n번 이상 나타난다.
  * <mark style="color:orange;">{n,m}</mark> : 정확한 반복횟수의 범위를 알고있을 때 표현한다.
    * x{n,.m} → x가 n번에서 m번 만큼 나타난다.
* <mark style="color:orange;">**.**</mark>** : 매우 강력한 표현. 어떤 글자던 다 포함한다.**
* <mark style="color:orange;">**^**</mark>**과 **<mark style="color:orange;">**$**</mark>** : ‘\[’과’]’안에 포함되지 않은 ‘^’은 라인의 시작을 의미하며 ‘$’은 라인의 종료를 의미한다.**
  * ^x$ → (start of line) x (end of line)
* **지정문자**
  * <mark style="color:orange;">**\s**</mark> : 공백문자
  * <mark style="color:orange;">**\S**</mark> : 공백문자를 제외한 모든 문자
  * <mark style="color:orange;">**\w**</mark> : alphanumeric(알파벳+숫자)+’_’ (= \[A-Za-z0-9_])
  * <mark style="color:orange;">**\W**</mark> : nonalphnumeric 문자 및 ‘_’ 제외(=\[^A-Za-z0-9_])
  * <mark style="color:orange;">**\d**</mark> : 숫자(=\[0-9])
  * <mark style="color:orange;">**\D**</mark> : 숫자를 제외한 모든 문자 (=\[^0-9])

```python
# 파이썬에서 정규표현식 사용

# Hello Ki, I would like to introduce regular expression in this section
# ~~
# Thank you!
# Sincerely,
# Ki: +82-10-1234-5678
# ▲ 위 텍스트에서 마지막줄을 모든 경우에 대해 정제하는 정규표현식을 짜보자.

# 마지막줄을 보고 아래 규칙을 생각해 볼 수 있다.
# - 이름이 전화번호 앞에 나올 수도 있다.
# - 이름 뒤에는 콜론이 나올 수도 있다.
# - 콜론 앞/뒤로는 (탭을 포함한) 공백이 다수 존재할 수도 있다.
# - 전화번호는 국가번호를 포함할 수도 있다.
# - 국가번호는 최대 3자리이다.
# - 국가번호의 앞에는 '+'가 붙을 수도 있다.
# - 전화번호 사이에 '-'가 들어갈 수도 있다.
# - 전화번호는 빈칸 없이 표현된다.
# - 전화번호 맨 앞과 지역번호(또는 010)의 다음에는 괄호가 들어갈 수도 있다.
# - 괄호는 한쪽만 나올 수도 있다.
# - 지역번호 자리의 맨 처음에 나오는 0은 빠질 수도 있다. 즉, 2자리가 될 수도 있따.
# - 지역번호 다음 번호 그룹은 3에서 4자리 숫자이다.
# - 마지막은 항상 4자리 숫자이다.


import re
regex = r"([\w]+\s*:?\s*)?\(?\+?([0-9]{1,3})?\-?[0-9]{2,3}(\)|\-)?[0-9]{3,4}\-?[0-9]{4}"

x = "Ki: +82-10-1234-5678"
re.sub(regex, "REMOVED", x) # REMOVED
```

사실 앞선 2개 경우는 초기 NLP연구에서 수행되었던 방법이고 현재에 와서는 하고자하는 업무와 수집한 데이터의 특징에 맞게 전처리해주는 경우가 대부분인듯 하다. **일반적으로 불필요한 노이즈를 제거하고, 형태를 일정하게 맞추어 주거나 하는 등의 전처리를 수행하게 된다.**



## Ⅱ. Stopwords

Corpus를 수집한 뒤 정제를 할 때 노이즈를 제거한다고 얘기했습니다. 여기서 노이즈는 불필요한 의미를 가진 단어들을 말한것입니다. 이러한 노이즈를 불용어(Stopwords)라고 부르게 됩니다. 예를 들면, I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 대표적인 경우입니다. 물론 불용어는 개발자가 직접 정의할 수도 있지만 여기서는 한국어 전처리에 초점을 맞추어 코드 구현을 다뤄보겠습니다.

{% code lineNumbers="true" %}
```python
from konlpy.tag import Okt

okt = Okt()

example = "고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지."
stop_words = "를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는"

stop_words = set(stop_words.split(' '))
word_tokens = okt.morphs(example)

result = [word for word in word_tokens if not word in stop_words]

print('불용어 제거 전 :',word_tokens) 
print('불용어 제거 후 :',result)
```
{% endcode %}

```
불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']
불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']
```

불용어가 많은 경우에는 코드 내에서 직접 정의하지 않고 txt 파일이나 csv 파일로 정리해놓고 이것을 불러와서 사용하기도 합니다. 다만 기준에 따라 내용은 바뀔 수 있으며 이 기준은 어떤 목적의 어떤 작업을 하느냐에 따라 달라집니다. 아래 링크는 가장 대표적인 한국어 불용어 리스트를 참조할 수 있는 사이트이니 참고하시기 바랍니다.

* [https://www.ranks.nl/stopwords/korean](https://www.ranks.nl/stopwords/korean)

## Ⅲ. Paddings

자연어 처리를 하다보면 각 문장(또는 문서)은 서로 길이가 다를 수 있습니다. 그런데 기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있습니다. 다시 말해 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요할 때가 있습니다. 이와 같이 데이터에 특정 값을 채워서 데이터의 크기(shape)를 조정하는 것을 패딩(padding)이라고 합니다. 숫자 0을 사용하고 있다면 **제로 패딩(zero padding)**이라고 합니다.

