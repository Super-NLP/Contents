---
description: 모델을 이용한 추론을 하는 방법과 최적화 기법을 알아봅시다.
coverY: 0
---

# Inference

* **vLLM : A high-throughput and memory-efficient inference and serving engine for LLMs**
  * [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)
* **FlashAttention : Fast and memory-efficient exact attention**
  * [https://github.com/Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)
*
