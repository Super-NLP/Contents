---
description: NLP 분야에서 사용되는 용어를 정리해봅시다!
coverY: 0
---

# Introduce

* **Backbone Model (백본 모델)**
  * 백본모델은 주로 딥러닝 기반의 이미지 처리나 자연어 처리 작업에서 기본적으로 사용되는 신경망 모델을 의미합니다. 이 모델은 다른 여러 계층이나 구조를 지원하는 기본 구조로서 작용합니다. 예를 들어, 컴퓨터 비전에서의 백본모델은 이미지의 특징을 추출하는 역할을 하며, 이렇게 추출된 특징은 다른 네트워크 구조나 머신러닝 알고리즘에 의해 추가적으로 처리됩니다. 자연어 처리에서도 백본모델은 문장이나 단어의 벡터 표현을 생성하는 기본 역할을 합니다. 즉, 백본모델은 특정 작업의 기본적인 특징을 추출하는 역할에 초점을 맞춘 모델이라 할 수 있습니다. 다만 실무환경에서는 Foundation Model과 혼용하여 용어를 사용하기도 합니다.
* **Foundatation Model (파운데이션 모델)**
  * 파운데이션 모델은 최근에 더 널리 사용되기 시작한 용어로, 대규모 데이터셋을 사용하여 사전 훈련된 범용 모델을 의미합니다. 이 모델들은 매우 큰 규모의 데이터셋에서 사전 훈련되어 다양한 다운스트림 작업(예: 텍스트 분류, 요약, 질문 응답 등)에 적용될 수 있습니다. 파운데이션 모델의 대표적인 예로는 GPT-3, BERT, T5 등이 있습니다. 이 모델들은 다양한 NLP 작업에 적응할 수 있는 범용성을 지니며, 특정 작업에 맞게 미세 조정될 수 있습니다. 즉, 파운데이션 모델은 다양한 작업에 적용 가능한 범용적이고 사전 훈련된 대규모 모델이라 할 수 있습니다. 다만 실무환경에서는 Backbone Model과 혼용하여 용어를 사용하기도 합니다.
* **Supervised Fine Tuning (SFT)**
  * 사전 훈련된 모델을 새로운, 레이블이 지정된 데이터셋으로 추가 훈련하여 특정 작업을 더 효과적으로 수행하도록 모델의 매개변수를 조정하는 과정입니다. 이 기술은 다양한 도메인에서 AI 모델의 정확성과 효율성을 향상시키는 데 널리 사용됩니다. 감독된 미세 조정은 레이블이 지정된 데이터를 사용하여 모델을 조정하는 과정에서 명확한 피드백을 제공하여 더 집중적인 학습을 가능하게 하고, 특정 작업에 대한 모델 성능을 향상시킵니다. 이 방법은 이미 언어에 대한 상당한 지식을 갖고 있는 사전 훈련된 모델을 활용하고, 최소한의 추가 훈련으로 원하는 애플리케이션에 적응시킬 수 있습니다.
* **Parameter-Efficient Fine Tuning (PEFT)**
  * 전체 모델 파라미터를 업데이트하는 대신 모델의 일부만을 업데이트하여 특정 작업에 대해 사전 훈련된 언어 모델을 미세 조정하는 방법입니다. 이 접근 방식은 기본적인 언어 이해를 그대로 유지하면서, 작업 특정 레이어나 어댑터를 추가하여 미세 조정합니다. PEFT의 주요 장점은 전체 미세 조정에 비해 계산 비용을 상당히 줄일 수 있다는 것입니다. PEFT 방식은 기존의 사전 훈련된 언어 모델의 가중치를 고정한 채로, 특정 작업에 대한 학습을 위해 추가된 부분만을 업데이트합니다. 이는 메모리 요구 사항을 크게 줄이고, 재해석 현상을 방지할 수 있으며, 다양한 작업에 대해 여러 모델을 동시에 미세 조정할 수 있는 가능성을 제공합니다. 대표적인 방법으로 LoRA가 있습니다.
* **GGML (GPT-Generated Model Language)**
  * GPT와 같은 언어 모델의 추론용으로 설계된 파일 형식입니다. 일종의 파일포맷으로 이해하면 됩니다. Georgi Gerganov가 개발한 GGML은 GPT 모델을 위한 파일 형식을 만들기 위한 초기 시도로, 단일 파일 공유와 CPU 호환성을 가능케 했지만, 모델 정보 추가에 어려움을 겪었으며 새로운 기능 도입시 기존 모델과의 호환성 문제, 그리고 사용자가 로프 빈도 기본값, 로프 빈도 스케일, GQA 및 RMS 정규화 엡실론과 같은 설정을 수동으로 조정해야 하는 복잡함이 있었습니다.
* **GGUF (GPT-Generated Unified Format)**
  * 2023년 8월에 발표된 GGUF는 언어 모델 파일 형식의 중요한 발전을 나타냅니다. Georgi Gerganov를 포함한 AI 커뮤니티의 기여자들이 개발한 GGUF는 GGML의 한계를 극복하기 위해 설계되었습니다. GGUF는 확장성, 안정성, 그리고 다양한 모델을 지원하는 등의 이점을 제공하며, llama 모델 이상의 다양한 모델을 지원합니다.
